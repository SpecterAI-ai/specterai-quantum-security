{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd7cca2f",
   "metadata": {},
   "source": [
    "# Code Example 1 — Classical Enumeration Baseline (MOVEit Threat Model)\n",
    "\n",
    "This notebook models a MOVEit-class breach as an **inference problem** under a classical attacker model.\n",
    "\n",
    "**Core idea:** The attacker must infer an internal \"branch/state\" of the target system using repeated interactions.  \n",
    "Each interaction increases certainty, but also increases exposure (more retries, more errors, more anomalies).\n",
    "\n",
    "We intentionally avoid:\n",
    "- payload construction\n",
    "- endpoint specifics\n",
    "- vendor targeting\n",
    "\n",
    "Instead, we simulate the *structure*:\n",
    "\n",
    "- A target has a hidden internal state `s*` among `N` candidates.\n",
    "- Each query returns a noisy \"score\" that correlates with correctness.\n",
    "- A classical attacker repeats queries to choose the best candidate with high confidence.\n",
    "\n",
    "Outputs produced here are designed to be captured in an exported HTML/PDF artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2d29996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade98c03",
   "metadata": {},
   "source": [
    "## Model and Parameters\n",
    "\n",
    "We define a hidden target state `s*` in a candidate set of size `N`.\n",
    "\n",
    "Each query produces a numeric score for each candidate:\n",
    "- The correct candidate has mean score `μ_good`\n",
    "- Incorrect candidates have mean score `μ_bad`\n",
    "- All scores include Gaussian noise `σ`\n",
    "\n",
    "A classical attacker must perform repeated queries and aggregate evidence.\n",
    "\n",
    "We report:\n",
    "- how many queries were required\n",
    "- how often the attacker succeeded\n",
    "- a simple \"exposure proxy\" that increases with query count\n",
    "- whether the process would likely trigger an anomaly threshold (illustrative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d331ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 123\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# Candidate space size (state/branch count)\n",
    "N = 128\n",
    "\n",
    "# Signal model: score separation\n",
    "mu_good = 1.0\n",
    "mu_bad  = 0.0\n",
    "sigma   = 1.0\n",
    "\n",
    "# How many queries an attacker will attempt at most\n",
    "max_queries = 200\n",
    "\n",
    "# Number of independent runs to estimate success probability\n",
    "trials = 200\n",
    "\n",
    "# Illustrative anomaly threshold: if queries exceed this, assume \"noisy\" behavior becomes noticeable\n",
    "anomaly_query_threshold = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec9c546",
   "metadata": {},
   "source": [
    "## Scoring Function\n",
    "\n",
    "Each query produces a score vector `scores[0..N-1]`.\n",
    "\n",
    "The \"correct\" internal state index `s*` has a slightly higher expected score.\n",
    "This is a simplified stand-in for real side-information (timing, response structure, error patterns, etc.).\n",
    "\n",
    "A classical attacker repeats queries and aggregates scores to identify the most likely state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22cb974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_scores(N, s_star, mu_good, mu_bad, sigma, rng):\n",
    "    \"\"\"\n",
    "    Generate one noisy score per candidate for a single query.\n",
    "    \"\"\"\n",
    "    scores = rng.normal(loc=mu_bad, scale=sigma, size=N)\n",
    "    scores[s_star] = rng.normal(loc=mu_good, scale=sigma)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2cc5a",
   "metadata": {},
   "source": [
    "## Classical Aggregation Strategy\n",
    "\n",
    "The attacker repeats queries and aggregates evidence using a simple running sum:\n",
    "\n",
    "- Initialize cumulative scores to zero\n",
    "- For each query:\n",
    "  - sample a new score vector\n",
    "  - add it to cumulative scores\n",
    "  - pick the current best candidate (argmax)\n",
    "\n",
    "We stop early once confidence is high enough.\n",
    "\n",
    "Confidence metric:\n",
    "- \"margin\" = best_score - second_best_score\n",
    "- Convert margin to a probability-like value via a logistic transform\n",
    "\n",
    "This is not a claim about real-world probabilities — it is a stable, printable way to show\n",
    "how classical methods require repeated interaction to reach high confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f534aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def confidence_from_margin(margin):\n",
    "    \"\"\"\n",
    "    Convert a margin to a smooth confidence value in (0,1).\n",
    "    \"\"\"\n",
    "    return logistic(margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5f136",
   "metadata": {},
   "source": [
    "## One Run Simulation\n",
    "\n",
    "We simulate a single attacker attempt.\n",
    "\n",
    "Stop condition:\n",
    "- confidence ≥ `target_confidence`, OR\n",
    "- reached `max_queries`\n",
    "\n",
    "We record:\n",
    "- queries used\n",
    "- whether the attacker guessed the correct state\n",
    "- exposure proxy (equal to queries used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4296e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_inference_run(N, mu_good, mu_bad, sigma, rng, max_queries=200, target_confidence=0.95):\n",
    "    # Hidden true state\n",
    "    s_star = int(rng.integers(0, N))\n",
    "    \n",
    "    cumulative = np.zeros(N)\n",
    "    queries_used = 0\n",
    "    conf = 0.0\n",
    "    \n",
    "    for t in range(1, max_queries + 1):\n",
    "        scores = sample_scores(N, s_star, mu_good, mu_bad, sigma, rng)\n",
    "        cumulative += scores\n",
    "        \n",
    "        # best and second best\n",
    "        best_idx = int(np.argmax(cumulative))\n",
    "        sorted_scores = np.sort(cumulative)\n",
    "        margin = sorted_scores[-1] - sorted_scores[-2]\n",
    "        conf = confidence_from_margin(margin)\n",
    "        \n",
    "        queries_used = t\n",
    "        \n",
    "        if conf >= target_confidence:\n",
    "            break\n",
    "    \n",
    "    success = (best_idx == s_star)\n",
    "    return {\n",
    "        \"success\": success,\n",
    "        \"queries_used\": queries_used,\n",
    "        \"confidence\": conf,\n",
    "        \"s_star\": s_star,\n",
    "        \"best_idx\": best_idx,\n",
    "        \"margin\": float(margin),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c61de95",
   "metadata": {},
   "source": [
    "## Batch Experiment (Classical Baseline)\n",
    "\n",
    "We run many trials to estimate:\n",
    "\n",
    "- success rate vs target confidence\n",
    "- typical query counts\n",
    "- fraction of runs exceeding a query threshold (proxy for anomaly risk)\n",
    "\n",
    "This is the classical attacker baseline: repeated interaction is required to collapse uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21cc282e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classical Enumeration Baseline ===\n",
      "Candidate states (N):                 128\n",
      "Signal separation (μ_good-μ_bad):     1.00\n",
      "Noise level (σ):                      1.00\n",
      "Target confidence:                    0.95\n",
      "Trials:                               200\n",
      "\n",
      "Success rate:                         0.895\n",
      "Median queries to reach confidence:   11\n",
      "90th percentile queries:              18\n",
      "Anomaly threshold (queries):          80\n",
      "Fraction exceeding anomaly threshold: 0.000\n"
     ]
    }
   ],
   "source": [
    "target_confidence = 0.95\n",
    "\n",
    "results = []\n",
    "for _ in range(trials):\n",
    "    r = classical_inference_run(\n",
    "        N=N,\n",
    "        mu_good=mu_good,\n",
    "        mu_bad=mu_bad,\n",
    "        sigma=sigma,\n",
    "        rng=rng,\n",
    "        max_queries=max_queries,\n",
    "        target_confidence=target_confidence\n",
    "    )\n",
    "    results.append(r)\n",
    "\n",
    "queries = np.array([r[\"queries_used\"] for r in results])\n",
    "successes = np.array([r[\"success\"] for r in results], dtype=int)\n",
    "confidences = np.array([r[\"confidence\"] for r in results])\n",
    "\n",
    "success_rate = successes.mean()\n",
    "median_queries = int(np.median(queries))\n",
    "p90_queries = int(np.percentile(queries, 90))\n",
    "anomaly_rate = (queries > anomaly_query_threshold).mean()\n",
    "\n",
    "print(\"=== Classical Enumeration Baseline ===\")\n",
    "print(f\"Candidate states (N):                 {N}\")\n",
    "print(f\"Signal separation (μ_good-μ_bad):     {mu_good - mu_bad:.2f}\")\n",
    "print(f\"Noise level (σ):                      {sigma:.2f}\")\n",
    "print(f\"Target confidence:                    {target_confidence:.2f}\")\n",
    "print(f\"Trials:                               {trials}\")\n",
    "print()\n",
    "print(f\"Success rate:                         {success_rate:.3f}\")\n",
    "print(f\"Median queries to reach confidence:   {median_queries}\")\n",
    "print(f\"90th percentile queries:              {p90_queries}\")\n",
    "print(f\"Anomaly threshold (queries):          {anomaly_query_threshold}\")\n",
    "print(f\"Fraction exceeding anomaly threshold: {anomaly_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc78a5e",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "This baseline demonstrates the **classical tradeoff**:\n",
    "\n",
    "- Higher confidence requires more queries.\n",
    "- More queries increases exposure and the likelihood of observable anomalies.\n",
    "- Even if each query is valid and well-formed, repeated probing creates detectable patterns.\n",
    "\n",
    "This is the baseline we will compare against in Code Example 2, where we model\n",
    "certainty compression (fewer interactions for equivalent confidence).\n",
    "\n",
    "Reference: Code Example 1 — Classical Enumeration Baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
